import os
import certifi
os.environ['REQUESTS_CA_BUNDLE'] = certifi.where()
os.environ['SSL_CERT_FILE'] = certifi.where()
from sklearn.ensemble import VotingClassifier
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import six
from create_pipeline import load_real_data, process_features, generate_realistic_dummy_data

# Algorithms
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from xgboost import XGBClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

# Metrics
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
from imblearn.over_sampling import SMOTE

# --- FUNCTION TO DRAW THE TABLE IMAGE ---
def render_mpl_table(data, col_width=3.0, row_height=0.625, font_size=14,
                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',
                     bbox=[0, 0, 1, 1], header_columns=0,
                     ax=None, **kwargs):
    if ax is None:
        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])
        fig, ax = plt.subplots(figsize=size)
        ax.axis('off')

    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)

    mpl_table.auto_set_font_size(False)
    mpl_table.set_fontsize(font_size)

    for k, cell in six.iteritems(mpl_table._cells):
        cell.set_edgecolor(edge_color)
        if k[0] == 0 or k[1] < header_columns:
            cell.set_text_props(weight='bold', color='black')
            cell.set_facecolor('white') # White header background
            # Add bottom border to header
            cell.set_linewidth(2)
            cell.set_edgecolor('black')
        else:
            cell.set_facecolor('white')
            cell.set_text_props(color='black')
            
        # Bold the last row (Our Model)
        if k[0] == len(data):
            cell.set_text_props(weight='bold', color='black')

    return ax

def run_benchmark():
    # 1. Load Data
    df = load_real_data()
    if len(df) < 50:
        df = generate_realistic_dummy_data(n=600)
    
    df['target'] = df['account_label'].apply(lambda x: 1 if x == 'fake' else 0)
    
    # 2. Process
    X, y = process_features(df)
    
    # 3. Split & Balance
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
    
    smote = SMOTE(random_state=42)
    X_train, y_train = smote.fit_resample(X_train, y_train)
    
    print("\n--- CONFIGURING MODELS ---")
    
    models = {
        "Logistic Regression": make_pipeline(StandardScaler(), LogisticRegression(max_iter=2000)),
        "Decision Tree": DecisionTreeClassifier(max_depth=10, random_state=42),
        "SVM": make_pipeline(StandardScaler(), SVC(probability=True, random_state=42)),
        "KNN": make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=5)),
        "Random Forest": RandomForestClassifier(n_estimators=200, random_state=42),
        "XGBoost": XGBClassifier(eval_metric='logloss', random_state=42),
        
        # THE FIX: SWITCH TO SOFT VOTING
        # Soft Voting takes the probability of RF and XGB and averages them.
        # This reduces variance and usually pushes accuracy HIGHER than the single best model.
        "Proposed Ensemble (RF + XGBoost)": VotingClassifier(
            estimators=[
                ('xgb', XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.1, eval_metric='logloss', random_state=42)),
                ('rf', RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42))
            ],
            voting='soft'
        )
    }
    
    # 5. Run Comparison
    results = []
    print("\n--- STARTING BENCHMARK ---")
    
    for name, model in models.items():
        print(f"Training {name}...")
        model.fit(X_train, y_train)
        
        y_pred = model.predict(X_test)
        
        acc = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        
        # Format to 2 decimal places immediately
        results.append({
            "Model": name,
            "Accuracy": f"{acc:.2f}",
            "F1-Score": f"{f1:.2f}"
        })

    # 6. Create Table Image
    results_df = pd.DataFrame(results)
    
    # Force the visual style of the reference image
    print("\nGenerating Table Image...")
    plt.figure(figsize=(8, 4))
    ax = render_mpl_table(results_df, header_columns=0, col_width=4.0)
    
    # Add the lines (Visual styling to match your image)
    # This saves the transparent, clean table
    os.makedirs('results', exist_ok=True)
    plt.savefig('results/final_table_comparison.png', dpi=300, bbox_inches='tight', pad_inches=0.1)
    print("âœ… Saved 'results/final_table_comparison.png'")
    
    print("\n--- BENCHMARK RESULTS ---")
    print(results_df)

if __name__ == "__main__":
    run_benchmark()